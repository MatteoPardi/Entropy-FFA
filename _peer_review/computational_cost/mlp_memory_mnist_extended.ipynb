{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a08dc49",
   "metadata": {
    "id": "4a08dc49"
   },
   "source": [
    "# Memory Consuming. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87379ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../_tools/')\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as trasf\n",
    "from utils.tensordata import TDataset, TDataloader, PosNeg_Bootstrap_TDataloader\n",
    "from _mlp import MLP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "th.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "th.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dadaea4",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f7c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_torch_dataset = torchvision.datasets.MNIST(\n",
    "    root=r\"C:\\Users\\matte\\LocalData\\Master Thesis\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=trasf.ToTensor()\n",
    ")\n",
    "TS_torch_dataset = torchvision.datasets.MNIST(\n",
    "    root=r\"C:\\Users\\matte\\LocalData\\Master Thesis\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=trasf.ToTensor()\n",
    ")\n",
    "\n",
    "# Load everything\n",
    "DS = TDataset(\n",
    "    x=th.stack([x.flatten() for x, y in DS_torch_dataset]).to(device),\n",
    "    y=th.tensor([y for x, y in DS_torch_dataset], device=device).reshape(-1, 1)\n",
    ")\n",
    "TS = TDataset(\n",
    "    x=th.stack([x.flatten() for x, y in TS_torch_dataset]).to(device),\n",
    "    y=th.tensor([y for x, y in TS_torch_dataset], device=device).reshape(-1, 1)\n",
    ")\n",
    "TR, VL = DS.random_split(5/6)\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 128\n",
    "DS_dl = DS.dataloader(batch_size=batch_size, method='shuffle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74216c3",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127c40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439db02",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d56cd",
   "metadata": {},
   "source": [
    "Warm up model. As first model to test, a trash one will be used, to be sure that every pytorch 'just-first-time' memory allocations have been allocated when interesting models are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd1c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_warmup = {\n",
    "    'task': 'classification',\n",
    "    'archit': (784, 10),\n",
    "    'f_hid': nn.ReLU(),\n",
    "    'weight_decay': 1e-3,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.99,\n",
    "    'n_epochs': 2\n",
    "}\n",
    "\n",
    "exps = {}\n",
    "exps['warmup'] = hyp_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4390e7",
   "metadata": {},
   "source": [
    "MLP models (hyp are not tuned... we're only interested in the memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b8f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_20 = {\n",
    "    'task': 'classification',\n",
    "    'archit': (784, 20, 20, 20, 20, 20, 20, 10),\n",
    "    'f_hid': nn.ReLU(),\n",
    "    'weight_decay': 1e-3,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.99,\n",
    "    'n_epochs': 2\n",
    "}\n",
    "hyp_200 = {\n",
    "    'task': 'classification',\n",
    "    'archit': (784, 200, 200, 200, 200, 200, 200, 10),\n",
    "    'f_hid': nn.ReLU(),\n",
    "    'weight_decay': 1e-3,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.99,\n",
    "    'n_epochs': 2\n",
    "}\n",
    "hyp_2000 = {\n",
    "    'task': 'classification',\n",
    "    'archit': (784, 2000, 2000, 2000, 2000, 2000, 2000, 10),\n",
    "    'f_hid': nn.ReLU(),\n",
    "    'weight_decay': 1e-3,\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.99,\n",
    "    'n_epochs': 2\n",
    "}\n",
    "\n",
    "exps |= {\n",
    "    'mlp20': hyp_20,\n",
    "    'mlp200': hyp_200,\n",
    "    'mlp2000': hyp_2000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cdccb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a402a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup: (baseline: 210.993 MB) 1.31885 MB\n",
      "mlp20: (baseline: 211.451 MB) 0.991211 MB\n",
      "mlp200: (baseline: 211.451 MB) 5.19775 MB\n",
      "mlp2000: (baseline: 211.451 MB) 274.701 MB\n"
     ]
    }
   ],
   "source": [
    "class MemoryAnalyzer:\n",
    "    \n",
    "    def __init__ (self, device):\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset (self):\n",
    "\n",
    "        th.cuda.reset_peak_memory_stats(device)\n",
    "        th.cuda.synchronize(device)\n",
    "        self._base_value = th.cuda.max_memory_allocated(device) \n",
    "    \n",
    "    def get_max_gap (self):\n",
    "        \n",
    "        return (th.cuda.max_memory_allocated(device) - self._base_value)/1024/1024 #MByte\n",
    "  \n",
    "ma = MemoryAnalyzer(device)\n",
    "for exp, hyp in exps.items():\n",
    "    print(exp, end=': ')\n",
    "    ma.reset()\n",
    "    print(f\"(baseline: {ma._base_value/1024/1024:.6g} MB)\", end=' ')\n",
    "    m = Model(hyp).to(device)\n",
    "    m.fit(DS_dl)\n",
    "    memory = ma.get_max_gap()\n",
    "    print(f\"{memory:.6g} MB\")\n",
    "    del m, memory"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
